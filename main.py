# -------------------------------
# BUILT-IN MODULES
import os
import re
import time

# THIRD_PARTY MODULES
import requests
import urllib.parse
from bs4 import BeautifulSoup
# -------------------------------

# Base URL of the website to scrape
website_url = ""
time_between_download_requests = 1  # Delay between requests (in seconds)
output_directory = os.path.join(
    os.getcwd(), "data"
)  # Create 'data' directory in script folder

# Regex to ignore resized images generated by WordPress
ignore_sizes_regex = r"-\d+x\d+\.[a-z]+"

# Ensure output directory exists
if not os.path.exists(output_directory):
    os.makedirs(output_directory)


def download_file(url, save_path):
    """Downloads a file from a URL and saves it to the specified path."""
    try:
        response = requests.get(url, stream=True)
        if response.status_code == 200:
            with open(save_path, "wb") as f:
                f.write(response.content)
            print(f"File downloaded: {save_path}")
        else:
            print(f"Failed to download: {url} (Status Code: {response.status_code})")
    except Exception as e:
        print(f"Error downloading {url}: {e}")


def traverse_url_recursive(url, sleep_time=1):
    """Recursively traverses a URL and downloads media files."""

    # Ignore URLs that match the ignore pattern
    if re.search(ignore_sizes_regex, url):
        return

    try:
        # Check if the URL is an HTML page
        response = requests.head(url, allow_redirects=True)
    except Exception as e:
        print(f"Failed to reach URL {url}: {e}")
        return

    content_type = response.headers.get("Content-Type", "")

    if "html" in content_type:
        # Process HTML page and look for links
        try:
            response = requests.get(url)
            html_parsed = BeautifulSoup(response.text, "html.parser")
        except Exception as e:
            print(f"Error parsing HTML at {url}: {e}")
            return

        # Traverse links within the HTML
        for link in html_parsed.find_all("a", href=True):
            link_text = link.get_text(strip=True)
            if link_text not in {
                "Name",
                "Last modified",
                "Size",
                "Description",
                "Parent Directory",
            }:
                next_url = urllib.parse.urljoin(url, link["href"])
                traverse_url_recursive(next_url, sleep_time)

    else:
        # Download non-HTML content (e.g., images, videos, PDFs)
        time.sleep(sleep_time)

        # Derive file path within the data directory
        file_path = os.path.join(
            output_directory, urllib.parse.urlparse(url).path.lstrip("/")
        )

        # Create necessary directories if they don't exist
        os.makedirs(os.path.dirname(file_path), exist_ok=True)

        # Download file
        download_file(url, file_path)


if __name__ == "__main__":
    # Start scraping from the 'uploads' directory (common for WordPress sites)
    start_url = urllib.parse.urljoin(website_url, "wp-content/uploads/")
    traverse_url_recursive(start_url, time_between_download_requests)
